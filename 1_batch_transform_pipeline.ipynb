{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Transform on Amazon SageMaker Pipelines Integrated with PrestoDB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This notebook works best with the `Data Science 3.0` kernel on an `ml.t3.medium` instance type***.\n",
    "\n",
    "Run the [0_model_training_pipeline](./0_model_training_pipeline.ipynb) notebook prior to running the notebook. This notebook runs a batch transform using the model trained in the previous notebook. It does so by running the following steps:\n",
    "\n",
    "1. Extract the latest approved model from the SageMaker model registry.\n",
    "\n",
    "1. Read raw data for inference from PrestoDB and stores in an Amazon S3 bucket.\n",
    "\n",
    "1. Create a SageMaker pipeline with a data processing step and a batch transform step to provide inference on the data. The inference results are also stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Install the necessary boto3 and sagemaker libraries to initialize session\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import logging\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "import sagemaker.session\n",
    "from typing import Dict, List\n",
    "from datetime import datetime, timedelta\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from utils import load_config, print_pipeline_execution_summary\n",
    "\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.processing import  ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## set the logger to track all of the logs as this pipeline runs\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Config.yml file that contains information that is used across this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = load_config('config.yml')\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## initialize the sagemaker session, region, role bucket and pipeline session\n",
    "session = sagemaker.session.Session()\n",
    "region = session.boto_region_name\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "ci = boto3.client('sts').get_caller_identity()\n",
    "\n",
    "role_name = config['aws']['sagemaker_execution_role_name']\n",
    "config['aws']['sagemaker_execution_role_arn'] = config['aws']['sagemaker_execution_role_arn'].format(account_id=ci['Account'], role=role_name)\n",
    "role = config['aws']['sagemaker_execution_role_arn']\n",
    "\n",
    "bucket = config['aws']['s3_bucket'].format(account_id=ci['Account'], region=region)\n",
    "prefix = config['aws']['s3_prefix']  # Prefix to S3 artifacts\n",
    "\n",
    "logger.info(f\"bucket={bucket}, prefix={prefix}, role={role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert your list to a JSON string\n",
    "training_features_str = json.dumps(config['training_step']['training_features'])\n",
    "logger.info(f\"the training features being used for this pipeline --> {training_features_str}\")\n",
    "\n",
    "# Define new pipeline parameters\n",
    "host_parameter = ParameterString(name=\"HostParameter\", default_value=config['presto']['host'])\n",
    "port_parameter = ParameterString(name=\"PortParameter\", default_value=config['presto']['parameter'])\n",
    "target_parameter = ParameterString(name=\"Target\", default_value=config['training_step']['training_target'])\n",
    "feature_parameter = ParameterString(name=\"Feature\", default_value=training_features_str)\n",
    "\n",
    "## presto credential key and region pipeline parameters\n",
    "presto_parameter = ParameterString(name=\"PrestoParameter\", default_value=config['presto']['presto_credentials'])\n",
    "region_parameter = ParameterString(name=\"Region\", default_value=config['aws']['region'])\n",
    "\n",
    "# query for the training job, write it to query_training.py\n",
    "fpath: str = os.path.join(config['scripts']['source_dir'], config['scripts']['query'])\n",
    "logger.info(f\"writing transform step query to {fpath}\")\n",
    "Path(fpath).write_text(f\"BATCH_INFERENCE_QUERY=\\\"\\\"\\\"{config['transform_step']['query']}\\\"\\\"\\\"\")\n",
    "\n",
    "## represents the parameters being used to track the catalog and the schema needed to connect to the presto server\n",
    "presto_catalog_parameter = ParameterString(name=\"Catalog\", default_value=config['presto']['catalog'])\n",
    "presto_schema_parameter = ParameterString(name=\"Schema\", default_value=config['presto']['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='parameters'></a>\n",
    "\n",
    "### Pipeline input parameters\n",
    "\n",
    "Pipeline Parameters are input parameter when triggering a pipeline execution. They need to be explicitly defined when creating the pipeline and contain default values.\n",
    "\n",
    "Create parameters for the inputs to the pipeline. In this case, parameters will be used for:\n",
    "\n",
    "- `ProcessingInstanceType` - What EC2 instance type to use for processing.\n",
    "- `TrainingInstanceType` - What EC2 instance type to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "# What instance type to use for processing.\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=config['data_processing_step']['processing_instance_type']\n",
    ")\n",
    "\n",
    "\n",
    "# Create SKlearn processor object,\n",
    "# The object contains information about what instance type to use, the IAM role to use etc.\n",
    "# A managed processor comes with a preconfigured container, so only specifying version is required.\n",
    "\n",
    "est_cls = sagemaker.sklearn.estimator.SKLearn\n",
    "\n",
    "sklearn_processor = FrameworkProcessor(\n",
    "                                     estimator_cls=est_cls,\n",
    "                                     framework_version=config['training_step']['sklearn_framework_version'],\n",
    "                                     role=role,\n",
    "                                     instance_type=processing_instance_type,\n",
    "                                     instance_count=config['data_processing_step']['instance_count'],\n",
    "                                     tags=config['data_processing_step']['tags'], \n",
    "                                     sagemaker_session=pipeline_session,\n",
    "                                     base_job_name=config['pipeline']['base_job_name'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an Image URI object to use while creating the model from the approved model in the registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch container to use for training\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"sklearn\",\n",
    "    region=config['aws']['region'],\n",
    "    version=config['training_step']['sklearn_framework_version'],\n",
    "    py_version=\"py3\",\n",
    "    instance_type=config['data_processing_step']['processing_instance_type'],\n",
    ")\n",
    "logger.info(f\"processing step image_uri={image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, step is to approve the model\n",
    "---\n",
    "Finally, approve the model to launch the model deployment process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "# list all model packages and select the latest one\n",
    "model_packages = []\n",
    "\n",
    "for p in sm.get_paginator('list_model_packages').paginate(\n",
    "        ModelPackageGroupName=config['register_model_step']['model_group'],\n",
    "        SortBy=\"CreationTime\",\n",
    "        SortOrder=\"Descending\",\n",
    "    ):\n",
    "    model_packages.extend(p[\"ModelPackageSummaryList\"])\n",
    "\n",
    "if len(model_packages) == 0:\n",
    "    raise Exception(f\"No model package is found for {config['register_model_step']['model_group']} model package group\")\n",
    "\n",
    "## print the latest model, approve it\n",
    "latest_model_package_arn = model_packages[0][\"ModelPackageArn\"]\n",
    "logger.info(f\"for model_group={config['register_model_step']['model_group']}, latest_model_package_arn={latest_model_package_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following statement sets the ModelApprovalStatus for the model package to Approved. The model package state change will launch the EventBridge rule and the rule will launch the CodePipeline CI/CD pipeline with model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## updating the latest model package to approved status to use it for batch inference\n",
    "model_package_update_response = sm.update_model_package(\n",
    "    ModelPackageArn=latest_model_package_arn,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Batch Transform Pipeline: Prepare Batch Data & Perform Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first step is to get the latest batch data from presto and use that for batch transform step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the sklearn_processor in a SageMaker Pipelines ProcessingStep\n",
    "# Configure the ProcessingStep\n",
    "\n",
    "## represents the output processing for the batch pre processing step\n",
    "batch_output=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"batch\",\n",
    "            source=\"/opt/ml/processing/batch\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "                    \"batch\",\n",
    "                ], \n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "# Use the sklearn_processor's run method and configure the batch preprocessing step\n",
    "step_args = sklearn_processor.run(\n",
    "    code=config['scripts']['batch_transform_get_data'],\n",
    "    source_dir=config['scripts']['source_dir'], \n",
    "    outputs=batch_output,\n",
    "    arguments=[\n",
    "        \"--host\", host_parameter,\n",
    "        \"--port\", port_parameter,\n",
    "        \"--presto_credentials_key\", presto_parameter,\n",
    "        \"--region\", region_parameter,\n",
    "        \"--presto_catalog\", presto_catalog_parameter,\n",
    "        \"--presto_schema\", presto_schema_parameter,\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "batch_data_prep = ProcessingStep(\n",
    "    name=config['data_processing_step']['step_name'],\n",
    "    step_args=step_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Transform Configuration begins below:\n",
    "---\n",
    "\n",
    "1. Create the model with the model image uri, refer to the 'inference.py' script that grabs information on features to use while making predictions.\n",
    "\n",
    "2. Create the model which automatically triggers the training and the preprocess data step\n",
    "\n",
    "3. Run the transformer step on the created model and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client(\"sagemaker\")\n",
    "list_model_packages_response = client.list_model_packages(ModelPackageGroupName=config['register_model_step']['model_group'])\n",
    "logger.info(f\"list_model_packages_response={list_model_packages_response}\")\n",
    "\n",
    "latest_model_version_arn = list_model_packages_response[\"ModelPackageSummaryList\"][0][\n",
    "    \"ModelPackageArn\"\n",
    "]\n",
    "logger.info(f\"latest_model_version_arn={latest_model_version_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    latest_approved_model_package = client.describe_model_package(ModelPackageName=latest_model_version_arn)\n",
    "\n",
    "    if latest_approved_model_package['ModelApprovalStatus'] == \"Approved\":\n",
    "        logger.info(f\"The latest approved model package is --> {latest_approved_model_package}\")\n",
    "        model_data_url = latest_approved_model_package['InferenceSpecification']['Containers'][0]['ModelDataUrl']\n",
    "        logger.info(f\"The model data for the latest approved model arn {latest_model_version_arn} is stored in {model_data_url}\")\n",
    "    else:\n",
    "        # If the model approval status is not PendingApproval, throw an error exception\n",
    "        error_message = f\"ModelApprovalStatus is not PendingApproval. Current status: {latest_approved_model_package['ModelApprovalStatus']}\"\n",
    "        logger.error(error_message)\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred while tracking the approved model: {str(e)}\")\n",
    "    raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## create the model image based on the model data and refer to the inference script as an entry point for \n",
    "## batch inference\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    entry_point=config['scripts']['batch_inference'],\n",
    "    model_data=model_data_url,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model image from the approved model for batch inference in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_create_model = ModelStep(\n",
    "    name=config['register_model_step']['model_name'],\n",
    "    step_args=model.create(instance_type=config['transform_step']['instance_type']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Transform Step to Perform Batch Transformation\n",
    "\n",
    "Now that a model instance is defined, create a Transformer instance with the appropriate model type, compute instance type, and desired output S3 URI.\n",
    "\n",
    "Specifically, pass in the ModelName from the CreateModelStep, step_create_model properties. The CreateModelStep properties attribute matches the object model of the DescribeModel response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Capture the current time for recording the start and end time for the batch transform step\n",
    "et = datetime.utcnow()\n",
    "st = et - timedelta(hours=config['transform_step']['num_hours_to_go_back'])\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=config['transform_step']['instance_type'],\n",
    "    instance_count=config['transform_step']['instance_count'],\n",
    "    strategy=\"MultiRecord\",\n",
    "    accept=\"text/csv\",\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=f\"s3://{bucket}\",\n",
    "    tags = config['transform_step']['tags'], \n",
    "    env={\n",
    "        'START_TIME_UTC': st.strftime('%Y-%m-%d %H:%M:%S'), \n",
    "        'END_TIME_UTC': et.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    }\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass in the transformer instance and the TransformInput with the batch_data pipeline parameter defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "# Assuming batch_prediction_data is the S3 path where your input data is stored\n",
    "transform_input = TransformInput(\n",
    "    data=batch_data_prep.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"batch\" ## this refers to the batch data that is configured within s3 after the batch preprocessing step\n",
    "            ].S3Output.S3Uri,\n",
    "    \n",
    "    content_type=\"text/csv\", \n",
    "    split_type=\"Line\")\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=config['transform_step']['step_name'], transformer=transformer, inputs=transform_input, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = config['pipeline']['transform_pipeline_name']\n",
    "\n",
    "batch_transform_pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[processing_instance_type,\n",
    "    host_parameter,\n",
    "    presto_parameter,\n",
    "    region_parameter,\n",
    "    port_parameter,\n",
    "    target_parameter, \n",
    "    feature_parameter,\n",
    "    presto_catalog_parameter,\n",
    "    presto_schema_parameter,],\n",
    "    steps=[\n",
    "        batch_data_prep,\n",
    "        step_create_model, \n",
    "        step_transform,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_transform_pipeline.upsert(role_arn=role, tags = config['pipeline']['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the sagemaker client to start the pipeline execution with the PipelineName referencing the previously created pipeline\n",
    "session = boto3.Session()\n",
    "sagemaker_client = session.client(service_name=\"sagemaker\")\n",
    "response = sagemaker_client.start_pipeline_execution(\n",
    "    PipelineName=batch_transform_pipeline.name\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.perf_counter()\n",
    "logger.info(f\"starting pipeline={batch_transform_pipeline.name}\")\n",
    "while True:\n",
    "    resp = client.describe_pipeline_execution(\n",
    "    PipelineExecutionArn=response['PipelineExecutionArn']\n",
    "    )\n",
    "    status = resp['PipelineExecutionStatus']\n",
    "    print(status)\n",
    "    time.sleep(30)\n",
    "    if status != 'Executing':\n",
    "        break\n",
    "elapsed_time = time.perf_counter() - st\n",
    "logger.info(f\"pipeline={batch_transform_pipeline.name} took {elapsed_time:.2f} seconds to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(json.dumps(resp, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
